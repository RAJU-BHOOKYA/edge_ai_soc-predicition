# -*- coding: utf-8 -*-
"""edge_ai_project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19HCSW1GnckdzLNAKxz3Gtcp6f0eRXjjs

for battery-1
"""

import pandas as pd
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import SimpleRNN, Dropout, Dense
from tensorflow.keras.callbacks import EarlyStopping
import numpy as np
import matplotlib.pyplot as plt

# Load the dataset
df = pd.read_csv("data file.csv")


# Show column names
print("Columns:", df.columns.tolist())

# Show column names and types
print("Columns:\n", df.columns)
print("\nData types:\n", df.dtypes)

# Show unique values in categorical columns (if any)
for col in df.columns:
    if df[col].dtype == "object":
        print(f"\nUnique values in '{col}':", df[col].unique()[:5])

# Check for missing values
print("\nMissing values:\n", df.isnull().sum())

# Show a sample row
df.iloc[0]

# Drop rows with missing values
df = df.dropna().reset_index(drop=True)

# Check for remaining missing values
print("\nMissing values:\n", df.isnull().sum())

df.shape

print("\nðŸ”¹ Summary statistics:\n", df.describe())

import seaborn as sns
numeric_cols = df.select_dtypes(include=['float64', 'int64']).columns

for col in numeric_cols:
    plt.figure(figsize=(6, 4))
    sns.histplot(df[col], kde=True, bins=30)
    plt.title(f'Distribution of {col}')
    plt.xlabel(col)
    plt.ylabel('Count')
    plt.grid(True)
    plt.tight_layout()
    plt.show()

plt.figure(figsize=(10, 8))
correlation_matrix = df[numeric_cols].corr()
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', linewidths=0.5)
plt.title("ðŸ”¸ Correlation Heatmap")
plt.show()

#pair plot
selected_features = numeric_cols[:5]
sns.pairplot(df[selected_features], diag_kind='kde')
plt.suptitle("ðŸ”¹ Pairwise Relationships", y=1.02)
plt.show()

from sklearn.preprocessing import MinMaxScaler


features = ['Voltage', 'Current', 'Temperature']
target = 'SOC_coloumb_count'


df_normalized = df.copy()


scaler = MinMaxScaler()

df_normalized[features] = scaler.fit_transform(df[features])

# Check the result
print("\nNormalized DataFrame:\n", df_normalized.head())

df = df_normalized

import numpy as np

# Define the sequence length
SEQ_LEN = 30

def create_sequences(data, seq_len):
    """
    Splits the dataset into sequences for time-series modeling.
    Args:
        data: 2D NumPy array containing all features and target
        seq_len: Length of each sequence
    Returns:
        X: 3D array of shape (num_sequences, seq_len, num_features)
        y: 1D array of target values
    """
    xs, ys = [], []
    for i in range(len(data) - seq_len):
        x = data[i:i + seq_len, :-1]
        y = data[i + seq_len, -1]
        xs.append(x)
        ys.append(y)
    return np.array(xs), np.array(ys)

# Prepare data for sequence creation
data_array = df[['Voltage', 'Current', 'Temperature', 'SOC_coloumb_count']].values

# Create sequences
X, y = create_sequences(data_array, SEQ_LEN)

# Check the shape of the resulting arrays
print("\nShape of X (features):", X.shape)
print("Shape of y (target):", y.shape)

from sklearn.model_selection import train_test_split

# Initial split: Train (80%) and Test (20%)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=False)

# Check shapes
print("Training set shape:", X_train.shape, y_train.shape)
print("Test set shape:", X_test.shape, y_test.shape)

# Further split training set into Train (80%) and Validation (20%)
X_train_final, X_val, y_train_final, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42, shuffle=False)

# Check shapes
print("Final Training set shape:", X_train_final.shape, y_train_final.shape)
print("Validation set shape:", X_val.shape, y_val.shape)

#RNN implementation

model = Sequential([
    SimpleRNN(128, return_sequences=True, input_shape=(X_train_final.shape[1], X_train_final.shape[2]), activation='tanh'),
    Dropout(0.2),
    SimpleRNN(128, activation='tanh'),
    Dropout(0.2),
    Dense(1, activation='linear')
])

# Compile the model
model.compile(optimizer='adam', loss='mse')

# Early stopping callback
early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)

# Train the model
history = model.fit(
    X_train_final, y_train_final,
    validation_data=(X_val, y_val),
    epochs=15,
    batch_size=64,
    callbacks=[early_stopping]
)

# Plot training and validation loss
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title("Training vs Validation Loss - RNN")
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.legend()
plt.show()

# Evaluate on test data
test_loss = model.evaluate(X_test, y_test)
print(f"Test Loss (RNN): {test_loss}")

#LSTM implementation

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dropout, Dense
from tensorflow.keras.callbacks import EarlyStopping
import numpy as np
import matplotlib.pyplot as plt

# Define the model with 128 LSTM units
model = Sequential([
    LSTM(128, return_sequences=True, input_shape=(X_train_final.shape[1], X_train_final.shape[2]), recurrent_activation='sigmoid'),
    Dropout(0.2),
    LSTM(128, recurrent_activation='sigmoid'),
    Dropout(0.2),
    Dense(1, activation='linear')
])

# Compile the model
model.compile(optimizer='adam', loss='mse')

# Early stopping callback
early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)

# Train the model
history = model.fit(
    X_train_final, y_train_final,
    validation_data=(X_val, y_val),
    epochs=15,
    batch_size=64,
    callbacks=[early_stopping]
)

# Plot training and validation loss
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title("Training vs Validation Loss")
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.legend()
plt.show()

# Evaluate on test data
test_loss = model.evaluate(X_test, y_test)
print(f"Test Loss: {test_loss}")

# Save the model in HDF5 format (compatible for TFLite conversion)
model.save("soc_model_data1.h5")

import tensorflow as tf
from tensorflow.keras.losses import MeanSquaredError


model = tf.keras.models.load_model("soc_model_data1.h5", custom_objects={"mse": MeanSquaredError()})


converter = tf.lite.TFLiteConverter.from_keras_model(model)


converter.experimental_enable_resource_variables = True


converter.optimizations = [tf.lite.Optimize.DEFAULT]


converter.target_spec.supported_types = [tf.float16]


converter.target_spec.supported_ops = [
    tf.lite.OpsSet.TFLITE_BUILTINS,
    tf.lite.OpsSet.SELECT_TF_OPS
]


converter._experimental_lower_tensor_list_ops = False

try:
    tflite_model = converter.convert()

    with open("soc_model_fp16_1.tflite", "wb") as f:
        f.write(tflite_model)
    print("Float16 TFLite model conversion successful!")
except Exception as e:
    print(f"TFLite model conversion failed: {e}")

!xxd -i soc_model_fp16.tflite > soc_model.cc

#same we trained the moel on second data set also





